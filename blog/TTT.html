<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Test-Time Training for Long Context - Yuming Lou</title>
  <link rel="stylesheet" href="../styles.css">
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
</head>
<body>

  <nav class="navbar">
    <div class="nav-container">
      <a href="../index.html" class="nav-logo">Yuming Lou</a>
      <ul class="nav-links">
        <li><a href="../index.html#about">About</a></li>
        <li><a href="../index.html#projects">Projects</a></li>
        <li><a href="../index.html#pubs">Publications</a></li>
        <li><a href="../index.html#blog" style="color:var(--primary-color);">Blog</a></li> 
      </ul>
    </div>
  </nav>

  <div class="main-container" style="margin-top: 40px;">
    
    <section>
      <a href="../index.html#blog" style="font-size: 0.9em; color: #666;">&larr; Back to Blog List</a>
      
      <div style="margin-top: 20px;">
        <h1 style="font-size: 2em; margin-bottom: 10px;">Test-Time Training for Long Context</h1>
        <p style="color: #888; font-style: italic;">Posted on Jan 24, 2026</p>
        
        <hr style="border: 0; border-top: 1px solid #eee; margin: 20px 0;">

        <article style="line-height: 1.8;">
          <h3>1. Introduction</h3>
          <p>Long-context capability is pivotal for modern machine learning, particularly in natural language processing and time series analysis. However, maintaining context over extended sequences remains a bottleneck. As shown in the figure below, processing a long document with the <a href="https://arxiv.org/abs/2505.09388" target="_blank">Qwen3-4B</a> model leads to an explosion in KV cache usage.</p>
          <figure style="text-align: center; margin: 20px 0;">
            <img src="../blog/serving/TTT/images/kv_cache_edge_device.png" alt="KV Cache Growth" style="max-width: 100%; height: auto; border: 1px solid #ccc; padding: 10px;">
            <figcaption style="font-size: 0.9em; color: #666; margin-top: 5px;">Figure: KV cache growth when processing long documents with Qwen3-4B.</figcaption>
          </figure>     

            <p>This contrasts sharply with biological intelligence. Unlike humans, whose hippocampus consolidates short-term working memory into long-term storage (often during sleep), LLMs rely solely on a growing raw buffer—the KV cache. Setting aside RAG (which acts more like an external notebook than an internal brain), I wonder if we can engineer a similar "memory consolidation" mechanism for LLMs. Specifically, can we leverage fine-tuning to compress history directly into model weights? By doing so, we could drastically reduce the KV cache footprint and improve inference speed. While training is computationally expensive, LLMs are not always active; we can potentially utilize the intervals between user queries—such as waiting for tool feedback or human response—to perform this optimization. This initial post explores the feasibility of this idea and the challenges that lie ahead.</p>
          
          <h3>2. Related Work: The Landscape of Long Context</h3>
            <p>
            To tackle the "memory wall" in LLMs, researchers have explored three main directions. 
            Here, I discuss why existing solutions might not be the final answer and how TTT fits into the picture.
            </p>

            <h4>2.1 Architectural Alternatives & The "Capacity Limit"</h4>
            <p>
            One direction is to design new memory forms, often inspired by RNNs. 
            <strong>Linear Attention</strong> mechanisms (like <a href="https://arxiv.org/abs/2305.13048" target="_blank">RWKV</a> or <a href="https://arxiv.org/abs/2312.00752" target="_blank">Mamba</a>) compress history into a fixed-size state, allowing for O(N) inference. 
            Google's recent <a href="https://arxiv.org/abs/2501.00663" target="_blank">Titans</a> also introduces a "Neural Memory" module that explicitly manages memorization and forgetting.
            </p>
            <p>
            However, these methods face a theoretical ceiling. As pointed out in this <a href="https://arxiv.org/abs/2102.11174" target="_blank">paper</a>, linear attention stores information in a matrix and retrieves it via dot products. 
            Crucially, effective retrieval requires the keys to be orthogonal.
            This implies that once the sequence length exceeds the dimension capacity (overcapacity regime), the model suffers from unavoidable retrieval errors due to interference. This explains why they often struggle with long context.
            </p>

            <h4>2.2 KV Cache Optimization: Pruning & Sparsity</h4>
            <p>
            For standard Transformers, another line of work focuses on managing the existing KV cache:
            </p>
            <ul>
            <li>
                <strong>Cache Eviction (Reducing Space):</strong> Methods like <a href="https://arxiv.org/abs/2404.14469" target="_blank">SnapKV</a>, <a href="#" target="_blank">H2O</a>, and <a href="https://arxiv.org/abs/2309.17453" target="_blank">StreamingLLM</a> identify and evict "unimportant" tokens to keep the cache size constant. While this accelerates inference and saves memory, it is inherently lossy—if the answer lies in an evicted token, the model hallucinates.
            </li>
            <li>
                <strong>Sparse Attention (Accelerating Compute):</strong> Other methods select only the top-K relevant tokens for calculation (e.g., <a href="https://arxiv.org/abs/2508.07101" target="_blank">Less Is More</a>). While this speeds up the attention operation, it doesn't necessarily reduce the memory footprint if the full cache still needs to be stored for selection.
            </li>
            </ul>

            <h4>2.3 Test-Time Training (TTT): The "Third Path"</h4>
            <p>
            This brings us to Test-Time Training (TTT). Analogous to the human brain, TTT allows the model to "learn" (update weights) from the context on-the-fly. 
            Recently, <a href="https://arxiv.org/abs/2512.23675" target="_blank">TTT-E2E </a> achieve to outperform linear attention trough test-time training. Specifically, they modify model architecture, optimize the training loss and train a model from scratch.
            </p>
            <p>
            However, TTT-E2E requires pre-training a novel architecture from scratch. This is computationally expensive and discards the massive knowledge embedded in existing open-source models.
            </p>
            <p>
            <strong>My Hypothesis:</strong>
            Can we leverage the TTT capability inherent in existing frozen LLMs? 
            Instead of designing a new TTT model, can we directly use TTT for existing models, thereby save space and accelerate inference?
            </p>
          
          <h3>3. Can model learn?</h3>
          <p>Recently I wrote some scripts to visualize the hidden states...</p>

          <h3>4. What have model learnt?</h3>
          <p>Recently I wrote some scripts to visualize the hidden states...</p>

          <h3>5. Future Work</h3>
          <p>There are several promising directions to explore:</p>

        </article>
        </div>
    </section>

  </div>

  <footer>
    <p>&copy; 2025 Yuming Lou.</p>
  </footer>

</body>
</html>